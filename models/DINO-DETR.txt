import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import Dinov2Model, Dinov2PreTrainedModel,Dinov2Config
from transformers.modeling_outputs import BaseModelOutput
from transformers import PreTrainedModel
from torchvision.ops import boxes as box_ops
import os
import json
import numpy as np
import matplotlib.pyplot as plt
from typing import Optional, Tuple, List, Dict
import sys # only for notebook
from PIL import Image
from collections import defaultdict
from torchvision import transforms



SCRIPT_DIR = os.getcwd()
DOTA_ROOT = os.path.abspath(os.path.join(SCRIPT_DIR, '..', '..', 'DOTA'))
TRAIN_IMAGE_DIR = os.path.join(DOTA_ROOT, 'training')  # or 'testing'
TEST_IMAGE_DIR = os.path.join(DOTA_ROOT,'testing')
CODE_ROOT=os.path.abspath(os.path.join(SCRIPT_DIR, '..', '..', 'Code'))
TRAIN_ANNOTATION_FILE = os.path.join(CODE_ROOT, 'annotations\dota_train_coco.json') 
TEST_ANNOTATION_FILE =  os.path.join(CODE_ROOT, 'annotations\dota_test_coco.json') 


class UAVDataset(Dataset):
    def __init__(self, img_dir, ann_file, transform=None):
        self.img_dir = img_dir
        self.transform = transform
        
        # Load the single annotation file
        with open(ann_file) as f:
            self.annotations = json.load(f)
        
        # Extract all image filenames and their annotations
        self.image_info = self.annotations['images']  # Assuming COCO format
        self.ann_info = self.annotations['annotations']
        self.categories = self.annotations['categories']
        
        # Create mappings
        self.class_id_to_idx = {cat['id']: idx+1 for idx, cat in enumerate(self.categories)}
        self.class_idx_to_name = {idx+1: cat['name'] for idx, cat in enumerate(self.categories)}
        self.num_classes = len(self.categories) + 1  # +1 for background
        
        # Build image to annotations mapping
        self.img_to_anns = defaultdict(list)
        for ann in self.ann_info:
            self.img_to_anns[ann['image_id']].append(ann)

    def __len__(self):
        return len(self.image_info)

    def __getitem__(self, idx):
        img_info = self.image_info[idx]
        img_path = os.path.join(self.img_dir, img_info['file_name'])
        img = Image.open(img_path).convert('RGB')
        
        anns = self.img_to_anns[img_info['id']]
        boxes = []
        labels = []
        
        for ann in anns:
            # COCO format: [x,y,width,height]
            x, y, w, h = ann['bbox']
            boxes.append([x, y, x + w, y + h])
            labels.append(self.class_id_to_idx[ann['category_id']])
        
        # Convert to tensors
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        labels = torch.as_tensor(labels, dtype=torch.int64)
        
        target = {
            'boxes': boxes,
            'labels': labels,
            'image_id': torch.tensor([img_info['id']]),
            'area': (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]),
            'iscrowd': torch.zeros((len(boxes),), dtype=torch.int64)
        }
        
        if self.transform:
            img, target = self.transform(img, target)
            
        return img, target

def initialize_config():
    # Create temporary dataset to get number of classes
    temp_dataset = UAVDataset(
        img_dir=TRAIN_IMAGE_DIR,
        ann_dir=TRAIN_ANNOTATION_FILE
    )
    config = {
        "pretrained_model_name": "facebook/dinov2-base",
        "num_classes": temp_dataset.num_classes,
        "hidden_dim": 256,
        "batch_size": 4,
        "num_epochs": 100,
        "learning_rate": 1e-4,
        "weight_decay": 1e-4,
        "device": torch.device("cuda" if torch.cuda.is_available() else "cpu"),
        "output_dir": "./output",
        "checkpoint_interval": 20,
        "resume_training": False,
        "class_names": temp_dataset.class_ids  # Optional: store original class IDs
    }
    
    print(f"Detected {config['num_classes']} classes (including background)")
    print(f"Class IDs: {temp_dataset.class_ids}")
    
    return config



class SmallObjectTransform:
    def __init__(self):
        self.transforms = transforms.Compose([
            transforms.Resize((1024, 1024)),  # Keep high resolution
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])
    
    def __call__(self, image, target):
        # Resize image and boxes
        orig_width, orig_height = image.size
        image = self.transforms[:2](image)  # Apply resize and ToTensor
        
        # Scale bounding boxes
        scale_x = 1024 / orig_width
        scale_y = 1024 / orig_height
        boxes = target['boxes']
        boxes[:, [0, 2]] *= scale_x
        boxes[:, [1, 3]] *= scale_y
        target['boxes'] = boxes
        
        # Apply normalization
        image = self.transforms[2](image)
        
        return image, target



class DINOv2DetrConfig(Dinov2Config):
    def __init__(self, num_classes=10, hidden_dim=256, **kwargs):
        super().__init__(**kwargs)
        self.num_classes = num_classes
        self.hidden_dim = hidden_dim

# Custom Model Class
class DINOv2ForDetection(PreTrainedModel):
    config_class = DINOv2DetrConfig
    
    def __init__(self, config):
        super().__init__(config)
        self.dinov2 = Dinov2Model(config)
        
        # Detection head
        self.classifier = nn.Linear(config.hidden_size, config.num_classes)
        self.bbox_regressor = nn.Linear(config.hidden_size, 4)
        
        # Initialize weights
        self.init_weights()
    
    def forward(self, pixel_values, labels=None, return_dict=None):
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
        
        # Get features
        outputs = self.dinov2(pixel_values=pixel_values, return_dict=return_dict)
        features = outputs.last_hidden_state if return_dict else outputs[0]
        
        # Global average pooling
        pooled_features = features.mean(dim=1)
        
        # Predictions
        logits = self.classifier(pooled_features)
        pred_boxes = self.bbox_regressor(pooled_features)
        
        # Convert to (x1, y1, x2, y2) format
        pred_boxes = self.decode_boxes(pred_boxes)
        
        loss = None
        if labels is not None:
            loss = self.compute_loss(logits, pred_boxes, labels)
        
        if not return_dict:
            output = (logits, pred_boxes) + outputs[2:]
            return ((loss,) + output) if loss is not None else output
        
        return {
            'loss': loss,
            'logits': logits,
            'pred_boxes': pred_boxes,
            'hidden_states': outputs.hidden_states,
            'attentions': outputs.attentions
        }
    
    def decode_boxes(self, boxes):
        """Convert network output to proper box coordinates"""
        boxes[:, 2:] = torch.exp(boxes[:, 2:])  # width, height
        boxes[:, 0] = boxes[:, 0] - boxes[:, 2] / 2  # x1
        boxes[:, 1] = boxes[:, 1] - boxes[:, 3] / 2  # y1
        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]      # x2
        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]      # y2
        return boxes
    
    def compute_loss(self, logits, pred_boxes, labels):
        # Classification loss
        class_loss = nn.functional.cross_entropy(
            logits, labels['labels'], reduction='mean')
        
        # Box regression loss
        box_loss = nn.functional.smooth_l1_loss(
            pred_boxes, labels['boxes'], reduction='mean')
        
        return class_loss + box_loss

# Modified Config class
class Config:
    def __init__(self):
        # Initialize datasets first to get num_classes
        train_dataset = UAVDataset(
            img_dir=TRAIN_IMAGE_DIR,
            ann_file=TRAIN_ANNOTATION_FILE
        )
        
        # Now set configuration
        self.train_img_dir = TRAIN_IMAGE_DIR
        self.train_ann_file = TRAIN_ANNOTATION_FILE
        self.val_img_dir = TEST_IMAGE_DIR
        self.val_ann_file = TEST_ANNOTATION_FILE
        
        # Model parameters
        self.model_config = DINOv2DetrConfig(
            num_classes=train_dataset.num_classes,
            hidden_dim=256
        )
        self.pretrained_model_name = "facebook/dinov2-base"
        self.image_size = 1024
        
        # Training parameters
        self.batch_size = 4
        self.num_epochs = 100
        self.learning_rate = 1e-4
        self.weight_decay = 1e-4
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Checkpointing
        self.output_dir = "./output"
        self.checkpoint_interval = 20
        self.resume_training = False
        self.resume_checkpoint = None

# Initialize model function
def initialize_model(config):
    # Load pre-trained DINOv2 with our custom config
    model = DINOv2ForDetection.from_pretrained(
        config.pretrained_model_name,
        config=config.model_config,
        ignore_mismatched_sizes=True
    )
    
    # Adjust for small objects
    model.config.num_attention_heads = 8
    model.config.image_size = config.image_size
    
    model.to(config.device)
    return model




# Training Utilities
class LossTracker:
    def __init__(self):
        self.epochs = []
        self.train_losses = []
        self.val_losses = []
    
    def update(self, epoch, train_loss, val_loss):
        self.epochs.append(epoch)
        self.train_losses.append(train_loss)
        self.val_losses.append(val_loss)
    
    def plot(self, save_path=None):
        plt.figure(figsize=(10, 6))
        plt.plot(self.epochs, self.train_losses, label='Train Loss')
        plt.plot(self.epochs, self.val_losses, label='Val Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('Training Progress')
        plt.legend()
        plt.grid(True)
        
        if save_path:
            plt.savefig(os.path.join(save_path, 'loss_curve.png'))
        plt.show()

def collate_fn(batch):
    return tuple(zip(*batch))

def train_one_epoch(model, dataloader, optimizer, device):
    model.train()
    total_loss = 0.0
    
    for images, targets in dataloader:
        images = [img.to(device) for img in images]
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]
        
        # Forward pass
        outputs = model(images, targets)
        loss = outputs['loss']
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
    
    return total_loss / len(dataloader)
def validate(model, dataloader, device):
    model.eval()
    total_loss = 0.0
    
    with torch.no_grad():
        for images, targets in dataloader:
            images = [img.to(device) for img in images]
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]
            
            outputs = model(images, targets)
            total_loss += outputs['loss'].item()
    
    return total_loss / len(dataloader)

def save_checkpoint(epoch, model, optimizer, loss_tracker, config):
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss_tracker': loss_tracker,
        'config': config
    }
    
    os.makedirs(config.output_dir, exist_ok=True)
    path = os.path.join(config.output_dir, f'checkpoint_epoch_{epoch}.pth')
    torch.save(checkpoint, path)
    print(f'Saved checkpoint to {path}')

def load_checkpoint(path, device):
    checkpoint = torch.load(path, map_location=device)
    return checkpoint

def print_class_info(dataset):
    print("\n=== Dataset Class Information ===")
    print(f"Total classes (including background): {dataset.num_classes}")
    print("\nClass ID Mapping:")
    print("{:<8} {:<8} {:<20}".format("Index", "ID", "Class Name"))
    print("-" * 36)
    print("{:<8} {:<8} {:<20}".format(0, "N/A", "background"))
    
    for cat in sorted(dataset.categories, key=lambda x: x['id']):
        print("{:<8} {:<8} {:<20}".format(
            dataset.class_id_to_idx[cat['id']],
            cat['id'],
            cat['name']
        ))



def main():
    # Initialize configuration
    config = Config()
    
    # Initialize datasets
    train_dataset = UAVDataset(config.train_img_dir, config.train_ann_file, SmallObjectTransform())
    val_dataset = UAVDataset(config.val_img_dir, config.val_ann_file, SmallObjectTransform())
    
    # Update config with dataset info
    config.num_classes = train_dataset.num_classes
    config.class_info = {
        'id_to_idx': train_dataset.class_id_to_idx,
        'idx_to_name': train_dataset.class_idx_to_name
    }
    
    # Print class information
    print_class_info(train_dataset)
    
    # Create dataloaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=4
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        collate_fn=collate_fn,
        num_workers=4
    )
    
    # Initialize model
    model = DINOv2ForDetection.from_pretrained(
        config.pretrained_model_name,
        num_labels=config.num_classes,
        ignore_mismatched_sizes=True
    )
    model.to(config.device)
    
    # Optimizer
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config.learning_rate,
        weight_decay=config.weight_decay
    )
    
    # Loss tracker
    loss_tracker = LossTracker()
    
    # Resume training if specified
    start_epoch = 0
    if config.resume_training and config.resume_checkpoint:
        checkpoint = load_checkpoint(config.resume_checkpoint, config.device)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        loss_tracker = checkpoint['loss_tracker']
        start_epoch = checkpoint['epoch'] + 1
        print(f"Resuming training from epoch {start_epoch}")
    
    # Training loop
    for epoch in range(start_epoch, config.num_epochs):
        print(f"\nEpoch {epoch + 1}/{config.num_epochs}")
        
        # Train
        train_loss = train_one_epoch(model, train_loader, optimizer, config.device)
        print(f"Train Loss: {train_loss:.4f}")
        
        # Validate
        val_loss = validate(model, val_loader, config.device)
        print(f"Val Loss: {val_loss:.4f}")
        
        # Update tracker
        loss_tracker.update(epoch + 1, train_loss, val_loss)
        
        # Save checkpoint
        if (epoch + 1) % config.checkpoint_interval == 0 or (epoch + 1) == config.num_epochs:
            save_checkpoint(epoch + 1, model, optimizer, loss_tracker, config)
    
    # Save final model
    torch.save(model.state_dict(), os.path.join(config.output_dir, 'final_model.pth'))
    
    # Plot losses
    loss_tracker.plot(config.output_dir)

if __name__ == "__main__":
    main()


